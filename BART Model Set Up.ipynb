{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U  torch transformers apex torchsummary jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
    "from transformers.file_utils import (\n",
    "    add_code_sample_docstrings,\n",
    "    add_end_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    "    Seq2SeqQuestionAnsweringModelOutput,\n",
    "    Seq2SeqSequenceClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.models.bart.modeling_bart import (\n",
    "    BartLearnedPositionalEmbedding,\n",
    "    BartDecoderLayer,\n",
    "    BartPreTrainedModel,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_attn_mask_utils import (\n",
    "    _prepare_4d_attention_mask,\n",
    "    _prepare_4d_causal_attention_mask,\n",
    ")\n",
    "from transformers.activations import ACT2FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART Model Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedFeatureSpecificMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads, d_model, num_features):\n",
    "        \"\"\"\n",
    "        Initializes the VectorizedFeatureSpecificMultiHeadAttention module.\n",
    "\n",
    "        Parameters:\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_model (int): Dimensionality of the input feature space.\n",
    "        num_features (int): Number of distinct features / experts.\n",
    "        \"\"\"\n",
    "        super(VectorizedFeatureSpecificMultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_features = num_features\n",
    "\n",
    "        assert d_model % self.num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(\n",
    "            d_model, d_model\n",
    "        )  # Same as original since key is already feature-specific\n",
    "\n",
    "    def split_heads(self, x, batch_size, seq_len):\n",
    "        \"\"\"\n",
    "        Splits the last dimension of x into (num_heads, depth) and reshapes.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        seq_len (int): Sequence length for the reshaping process.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Reshaped tensor.\n",
    "        \"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.num_heads, self.depth)\n",
    "        x = x.reshape(*new_shape).permute(\n",
    "            0, 1, 3, 2, 4\n",
    "        )  # (num_features, batch_size, num_heads, seq_len, depth)\n",
    "        return x\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        \"\"\"\n",
    "        Forward pass for the VectorizedFeatureSpecificMultiHeadAttention.\n",
    "\n",
    "        Parameters:\n",
    "        query (torch.Tensor): Query tensor of shape (batch_size, seq_len_q, d_model).\n",
    "        key (torch.Tensor): Key tensor of shape (num_features, batch_size, seq_len_k, d_model).\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Concatenated attention weights across all heads and features.\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_q = query.size(1)\n",
    "        seq_len_k = key.size(2)\n",
    "\n",
    "        # Prepare query and key\n",
    "        query = self.split_heads(\n",
    "            self.wq(query), batch_size, seq_len_q\n",
    "        )  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        key = self.split_heads(\n",
    "            self.wk(key), batch_size, seq_len_k\n",
    "        )  # (num_features, batch_size, num_heads, seq_len_k, depth)\n",
    "\n",
    "        # Compute attention\n",
    "        attention_weights = self.compute_attention(query, key, seq_len_k)\n",
    "\n",
    "        return attention_weights\n",
    "\n",
    "    def compute_attention(self, query, key, seq_len_k):\n",
    "        \"\"\"\n",
    "        Computes the scaled dot-product attention.\n",
    "\n",
    "        Parameters:\n",
    "        query (torch.Tensor): Query tensor.\n",
    "        key (torch.Tensor): Key tensor.\n",
    "        seq_len_k (int): Key sequence length.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Attention weights.\n",
    "        \"\"\"\n",
    "        # Expand query to match key's features\n",
    "        # query = query.unsqueeze(0).expand(\n",
    "        #     self.num_features, -1, -1, -1, -1\n",
    "        # )  # (num_features, batch_size, num_heads, seq_len_q, depth)\n",
    "\n",
    "        # Perform batch matrix multiplication\n",
    "        matmul_qk = torch.matmul(\n",
    "            query, key.transpose(-2, -1)\n",
    "        )  # (num_features, batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # Scale\n",
    "        scale_factor = 1 / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n",
    "        attention_weights = matmul_qk * scale_factor\n",
    "\n",
    "        # # Apply softmax along the dimension of key's sequence length\n",
    "        # attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "\n",
    "        # Reshape to combine features and batch\n",
    "        # _, _, num_heads, _, _ = scaled_attention_logits.shape\n",
    "        # attention_weights = scaled_attention_logits.permute(\n",
    "        #     1, 2, 0, 3, 4\n",
    "        # ).contiguous()  # (batch_size, num_heads, num_features, seq_len_q, seq_len_k)\n",
    "\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scaled_dot_product_attention(query, key, d_k):\n",
    "#     \"\"\"\n",
    "#     Compute the scaled dot product attention weights.\n",
    "\n",
    "#     Args:\n",
    "#     query (Tensor): Query tensor of shape (num_replicas, batch_size,seq_len_query,  d_k) or (seq_len_query, batch_size, d_k) if not batched.\n",
    "\n",
    "#     key (Tensor): Key tensor of shape (num_replicas,  batch_size, seq_len_key, d_k) or (batch_size, seq_len_key, d_k) if not batched.\n",
    "#     d_k (int): Dimension of the key and query tensors.\n",
    "\n",
    "#     Returns:\n",
    "#     Tensor: Attention weights of shape (num_replicas, seq_len_query, batch_size, seq_len_key)\n",
    "#             or (seq_len_query, batch_size, seq_len_key) if not batched.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Compute the dot product between query and key\n",
    "#     scores = torch.matmul(\n",
    "#         query, key.transpose(-2, -1)\n",
    "#     )  # Shape: (num_replicas, batch_size, seq_len_query, seq_len_key)\n",
    "\n",
    "#     # Scale the scores\n",
    "#     attention_weights = scores / torch.sqrt(torch.tensor(d_k, dtype=scores.dtype))\n",
    "#     # print(\"query shape: \", query.shape)\n",
    "#     # print(\"key shape: \", key.shape)\n",
    "#     # print(\"attention_weights shape: \", attention_weights.shape)\n",
    "\n",
    "#     return attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBartDecoder(BartPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`BartDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "        embed_tokens (nn.Embedding): output embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
    "        super().__init__(config)\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.decoder_layerdrop\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.max_target_positions = config.max_position_embeddings\n",
    "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            config.vocab_size+1, config.d_model, self.padding_idx\n",
    "        )\n",
    "\n",
    "        if embed_tokens is not None:\n",
    "            self.embed_tokens.weight = embed_tokens.weight\n",
    "\n",
    "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
    "            config.max_position_embeddings,\n",
    "            config.d_model,\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [BartDecoderLayer(config) for _ in range(config.decoder_layers)]\n",
    "        )\n",
    "        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n",
    "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
    "                of the decoder.\n",
    "            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n",
    "                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n",
    "                selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n",
    "                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
    "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
    "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
    "\n",
    "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
    "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
    "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of\n",
    "                shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing\n",
    "                `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more\n",
    "                control over how to convert `input_ids` indices into associated vectors than the model's internal\n",
    "                embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\"\n",
    "            )\n",
    "        elif input_ids is not None:\n",
    "            input = input_ids\n",
    "            input_shape = input.shape\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            input = inputs_embeds[:, :, -1]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"You have to specify either decoder_input_ids or decoder_inputs_embeds\"\n",
    "            )\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = (\n",
    "            past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "        )\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input) * self.embed_scale\n",
    "\n",
    "        if getattr(self.config, \"_flash_attn_2_enabled\", False):\n",
    "            # 2d mask is passed through the layers\n",
    "            attention_mask = (\n",
    "                attention_mask\n",
    "                if (attention_mask is not None and 0 in attention_mask)\n",
    "                else None\n",
    "            )\n",
    "        else:\n",
    "            # 4d mask is passed through the layers\n",
    "            attention_mask = _prepare_4d_causal_attention_mask(\n",
    "                attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
    "            )\n",
    "\n",
    "        # expand encoder attention mask\n",
    "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
    "            if getattr(self.config, \"_flash_attn_2_enabled\", False):\n",
    "                encoder_attention_mask = (\n",
    "                    encoder_attention_mask if 0 in encoder_attention_mask else None\n",
    "                )\n",
    "            else:\n",
    "                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "                encoder_attention_mask = _prepare_4d_attention_mask(\n",
    "                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n",
    "                )\n",
    "\n",
    "        # embed positions\n",
    "        positions = self.embed_positions(input, past_key_values_length)\n",
    "        positions = positions.to(inputs_embeds.device)\n",
    "\n",
    "        hidden_states = inputs_embeds + positions\n",
    "        hidden_states = self.layernorm_embedding(hidden_states)\n",
    "\n",
    "        hidden_states = nn.functional.dropout(\n",
    "            hidden_states, p=self.dropout, training=self.training\n",
    "        )\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        all_cross_attentions = (\n",
    "            () if (output_attentions and encoder_hidden_states is not None) else None\n",
    "        )\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n",
    "        for attn_mask, mask_name in zip(\n",
    "            [head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]\n",
    "        ):\n",
    "            if attn_mask is not None:\n",
    "                if attn_mask.size()[0] != (len(self.layers)):\n",
    "                    raise ValueError(\n",
    "                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                        f\" {head_mask.size()[0]}.\"\n",
    "                    )\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            for idx, decoder_layer in enumerate(self.layers):\n",
    "                # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "                if output_hidden_states:\n",
    "                    all_hidden_states += (hidden_states,)\n",
    "                if self.training:\n",
    "                    dropout_probability = torch.rand([])\n",
    "                    if dropout_probability < self.layerdrop:\n",
    "                        continue\n",
    "\n",
    "                past_key_value = (\n",
    "                    past_key_values[idx] if past_key_values is not None else None\n",
    "                )\n",
    "\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "                    layer_outputs = self._gradient_checkpointing_func(\n",
    "                        decoder_layer.__call__,\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        encoder_hidden_states,\n",
    "                        encoder_attention_mask,\n",
    "                        head_mask[idx] if head_mask is not None else None,\n",
    "                        cross_attn_head_mask[idx]\n",
    "                        if cross_attn_head_mask is not None\n",
    "                        else None,\n",
    "                        None,\n",
    "                        output_attentions,\n",
    "                        use_cache,\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = decoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask=attention_mask,\n",
    "                        encoder_hidden_states=encoder_hidden_states,\n",
    "                        encoder_attention_mask=encoder_attention_mask,\n",
    "                        layer_head_mask=(\n",
    "                            head_mask[idx] if head_mask is not None else None\n",
    "                        ),\n",
    "                        cross_attn_layer_head_mask=(\n",
    "                            cross_attn_head_mask[idx]\n",
    "                            if cross_attn_head_mask is not None\n",
    "                            else None\n",
    "                        ),\n",
    "                        past_key_value=past_key_value,\n",
    "                        output_attentions=output_attentions,\n",
    "                        use_cache=use_cache,\n",
    "                    )\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "                if use_cache:\n",
    "                    next_decoder_cache += (\n",
    "                        layer_outputs[3 if output_attentions else 1],\n",
    "                    )\n",
    "\n",
    "                if output_attentions:\n",
    "                    all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "                    if encoder_hidden_states is not None:\n",
    "                        all_cross_attentions += (layer_outputs[2],)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attns,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )  ## Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bart EncoderLayer Modification\n",
    "# class CustomBartEncoderLayer(nn.Module):\n",
    "#     def __init__(self, config: BartConfig, layer):\n",
    "#         super().__init__()\n",
    "#         self.layer = layer\n",
    "#         num_replicas = 19\n",
    "#         self.num_replicas = num_replicas\n",
    "#         self.embed_dim = config.d_model\n",
    "\n",
    "#         self.fc1 = nn.ModuleList(\n",
    "#             [nn.Linear(self.embed_dim, self.embed_dim) for _ in range(num_replicas)]\n",
    "#         )\n",
    "#         # print(self.embed_dim, config.decoder_ffn_dim)\n",
    "#         # self.fc2 = nn.ModuleList(\n",
    "#         #     [nn.Linear(self.embed_dim, self.embed_dim) for _ in range(num_replicas)]\n",
    "#         # )\n",
    "\n",
    "#         self.q1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "#         self.k1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "#         # self.q2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "#         # self.k2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "#         # print(self.embed_dim, config.decoder_ffn_dim, config.decoder_attention_heads)\n",
    "\n",
    "#         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "#         self.dropout = config.dropout\n",
    "#         self.activation_fn = ACT2FN[config.activation_function]\n",
    "#         self.activation_dropout = config.activation_dropout  # Define activation dropout\n",
    "\n",
    "#     def forward(self, x, *args, **kwargs):\n",
    "#         outputs = self.layer(x, *args, **kwargs)\n",
    "#         hidden_states = outputs[0]\n",
    "#         residual = hidden_states\n",
    "#         hidden_states = self.activation_fn(hidden_states)\n",
    "\n",
    "#         batch_size = hidden_states.size(1)\n",
    "\n",
    "#         q1 = self.q1(hidden_states)\n",
    "#         fc1_concat = torch.stack([fc1(hidden_states) for fc1 in self.fc1], dim=0)\n",
    "#         p1 = torch.stack([fc1.weight.data for fc1 in self.fc1], dim=0)\n",
    "#         p1 = p1.unsqueeze(2).expand(-1, -1, batch_size, -1)\n",
    "#         k1 = self.k1(p1)\n",
    "#         attention_weight_1 = scaled_dot_product_attention(q1, k1, self.embed_dim)\n",
    "#         attention_weight_1 = nn.functional.softmax(attention_weight_1, dim=0)\n",
    "#         # print(self.num_replicas)\n",
    "#         # print(\"q1\", q1.shape)\n",
    "#         # print(\"k1\", k1.shape)\n",
    "#         # print(\"fc1_concat\", fc1_concat.shape)\n",
    "#         # print(\"attention_weight_1\", attention_weight_1.shape)\n",
    "#         fc1_weighted = fc1_concat * attention_weight_1\n",
    "#         hidden_states = fc1_weighted.sum(dim=0)\n",
    "#         # hidden_states = self.activation_fn(hidden_states)\n",
    "#         # print((attention_weight_1.sum(dim=(1,2,3))))\n",
    "\n",
    "#         # hidden_states = nn.functional.dropout(\n",
    "#         #     hidden_states, p=self.activation_dropout, training=self.training\n",
    "#         # )\n",
    "\n",
    "#         # # Vectorized operation for fc2 layers\n",
    "#         # q2 = self.q2(hidden_states)\n",
    "#         # fc2_concat = torch.stack([fc2(hidden_states) for fc2 in self.fc2], dim=0)\n",
    "#         # p2 = torch.stack([fc2.weight.data for fc2 in self.fc2], dim=0)\n",
    "#         # p2 = p2.unsqueeze(2).expand(-1, -1, batch_size, -1)\n",
    "#         # k2 = self.k2(p2)\n",
    "#         # attention_weight_2 = scaled_dot_product_attention(q2, k2, self.embed_dim)\n",
    "#         # fc2_weighted = fc2_concat * attention_weight_2\n",
    "#         # hidden_states = fc2_weighted.mean(dim=0)\n",
    "\n",
    "#         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "#         hidden_states = hidden_states + residual\n",
    "#         hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "#         return (hidden_states,) + outputs[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BartDecoderLayer Modification\n",
    "class CustomBartDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: BartConfig, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.num_replicas = num_replicas\n",
    "        self.embed_dim = config.d_model  # Assuming embed_dim is d_model\n",
    "        self.fc1 = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "                for _ in range(num_replicas)\n",
    "            ]\n",
    "        )\n",
    "        # print(self.embed_dim, config.decoder_ffn_dim)\n",
    "        # self.fc2 = nn.ModuleList(\n",
    "        #     [nn.Linear(self.embed_dim, self.embed_dim, bias=False) for _ in range(num_replicas)]\n",
    "        # )\n",
    "        self.q1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.attention_weight_bias = nn.Parameter(torch.zeros(num_replicas, 1, 1, 1))\n",
    "\n",
    "        # self.q2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.k2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        # print(self.embed_dim, config.decoder_ffn_dim, config.decoder_attention_heads)\n",
    "\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "        self.mh_attn = VectorizedFeatureSpecificMultiHeadAttention(\n",
    "            num_heads=config.decoder_attention_heads, d_model=self.embed_dim, num_features=self.num_replicas\n",
    "        )\n",
    "        # config.decoder_attention_heads\n",
    "        # for i, fc1 in enumerate(self.fc1):\n",
    "        #     # Initialize with a simple pattern, e.g., all elements in the weight matrix are set to the index of the layer\n",
    "        #     nn.init.constant_(fc1.weight, 17 * i)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        outputs = self.layer(x, *args, **kwargs)\n",
    "        hidden_states = outputs[0]\n",
    "        residual = hidden_states\n",
    "\n",
    "        batch_size = hidden_states.size(0)\n",
    "        seq_len = hidden_states.size(1)\n",
    "\n",
    "        p1 = torch.stack([fc1.weight for fc1 in self.fc1], dim=0)\n",
    "        # .transpose(0, 1)\n",
    "        p1 = p1.unsqueeze(1)\n",
    "        # .expand(-1, batch_size, -1, -1)\n",
    "\n",
    "        hidden_states_reshaped = hidden_states.unsqueeze(0).expand(\n",
    "            self.num_replicas, -1, -1, -1\n",
    "        )\n",
    "        fc1_concat = torch.matmul(hidden_states_reshaped, p1)\n",
    "        # fc1_concat1 = torch.stack([fc1(hidden_states) for fc1 in self.fc1], dim=0)\n",
    "\n",
    "        # print(\"p1\", p1.shape)\n",
    "        # print(\"hidden_states_reshaped\",hidden_states_reshaped.shape)\n",
    "        # print(\"fc1_concat1\",fc1_concat1.shape)\n",
    "        # print(\"fc1_concat\", fc1_concat.shape)\n",
    "        # torch.set_printoptions(sci_mode=False)\n",
    "        # print(\"Same:\", torch.allclose(fc1_concat, fc1_concat1, rtol = 0.001))\n",
    "        # print(torch.abs(fc1_concat - fc1_concat1).mean())\n",
    "        # print(torch.abs(fc1_concat - fc1_concat1).max())\n",
    "        # print()\n",
    "\n",
    "        # q1 = self.q1(hidden_states).unsqueeze(0).expand(self.num_replicas, -1, -1, -1)\n",
    "        # k1 = self.k1(p1)\n",
    "        # attention_weight_1 = scaled_dot_product_attention(q1, k1, self.embed_dim)        \n",
    "        # attention_weight_1 = attention_weight_1 + self.attention_weight_bias\n",
    "\n",
    "        # print(\"q1\", q1.shape)\n",
    "        # print(\"k1\", k1.shape)\n",
    "        mh_attn_weight = self.mh_attn(query=hidden_states_reshaped, key=p1) # shape: (num_experts, batch_size, num_heads, seq_len, embed_dim)\n",
    "        # print(\"attention_weight_1\", attention_weight_1.shape)\n",
    "        # print(\"mh_attn_weight\", mh_attn_weight.shape)\n",
    "        # print(\"Same:\", torch.allclose(attention_weight_1, mh_attn_weight, rtol=0.001))\n",
    "        # print(torch.abs(attention_weight_1 - mh_attn_weight).mean())\n",
    "        # print(torch.abs(attention_weight_1 - mh_attn_weight).max())\n",
    "        # print()\n",
    "\n",
    "        attention_weight_1_norm_expert = nn.functional.softmax(\n",
    "            mh_attn_weight, dim=0\n",
    "        ).mean(dim = 2)\n",
    "        attention_weight_1_norm_feature = nn.functional.softmax(\n",
    "            mh_attn_weight, dim=-1\n",
    "        ).mean(dim = 2)\n",
    "        attention_weight_1_combined = (\n",
    "            3 * attention_weight_1_norm_expert + attention_weight_1_norm_feature\n",
    "        )\n",
    "        # print(self.num_replicas)\n",
    "        # print(\"q1\", q1.shape)\n",
    "        # print(\"k1\", k1.shape)\n",
    "        # print(\"fc1_concat\", fc1_concat.shape)\n",
    "        # print(\"attention_weight_1\", attention_weight_1.shape)\n",
    "        fc1_weighted = fc1_concat * attention_weight_1_combined\n",
    "        hidden_states = (fc1_weighted.mean(dim=0))\n",
    "        # self.activation_fn\n",
    "        # print((attention_weight_1.sum(dim=(1,2,3))))\n",
    "        # hidden_states = nn.functional.dropout(\n",
    "        #     hidden_states, p=self.activation_dropout, training=self.training\n",
    "        # )\n",
    "\n",
    "        # # Vectorized operation for fc2 layers\n",
    "        # q2 = self.q2(hidden_states)\n",
    "        # fc2_concat = torch.stack([fc2(hidden_states) for fc2 in self.fc2], dim=0)\n",
    "        # p2 = torch.stack([fc2.weight.data for fc2 in self.fc2], dim=0)\n",
    "        # p2 = p2.unsqueeze(2).expand(-1, -1, batch_size, -1)\n",
    "        # k2 = self.k2(p2)\n",
    "        # attention_weight_2 = scaled_dot_product_attention(q2, k2, self.embed_dim)\n",
    "        # fc2_weighted = fc2_concat * attention_weight_2\n",
    "        # hidden_states = fc2_weighted.mean(dim=0)\n",
    "\n",
    "        hidden_states = nn.functional.dropout(\n",
    "            hidden_states, p=self.dropout, training=self.training\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states + residual\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        return (hidden_states,) + outputs[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): CustomBartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CustomBartDecoderLayer(\n",
       "          (layer): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (fc1): ModuleList(\n",
       "            (0-6): 7 x Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (q1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (mh_attn): VectorizedFeatureSpecificMultiHeadAttention(\n",
       "            (wq): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (wk): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained BART model\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "config = BartConfig.from_pretrained(model_name)\n",
    "# print(config)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "num_iterations = 2\n",
    "num_replicas = 7\n",
    "model.model.decoder = CustomBartDecoder(\n",
    "    config=model.config, embed_tokens=model.model.shared\n",
    ")\n",
    "\n",
    "# Check if CUDA GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "model_weights_path = f\"./model_weights/name.pth\"\n",
    "\n",
    "# # Load model weights to the device\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load(model_weights_path, map_location=\"cuda\"))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_weights_path, map_location=\"cpu\"))\n",
    "\n",
    "# Replace all customized layers\n",
    "for i, layer in enumerate(model.model.decoder.layers):\n",
    "    model.model.decoder.layers[i] = CustomBartDecoderLayer(model.config, layer)\n",
    "\n",
    "model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "# Move the model to the specified device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Warehouse\n",
    "# model = BartForConditionalGeneration(config=config)\n",
    "\n",
    "# for i, layer in enumerate(model.model.encoder.layers):\n",
    "#     model.model.encoder.layers[i] = CustomBartEncoderLayer(model.config, layer)\n",
    "\n",
    "\n",
    "# #save pretrained model weights\n",
    "# torch.save(model.state_dict(), model_weigts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 544777300\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze pretrained weight\n",
    "\n",
    "# Step 1: Freeze all pretrained weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Step 2: Unfreeze the weights in custom layers\n",
    "for i in range(len(model.model.decoder.layers)):\n",
    "    layer = model.model.decoder.layers[i]\n",
    "    if isinstance(layer, CustomBartDecoderLayer):\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if isinstance(layer, BartDecoderLayer):\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test (Generate Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProsecutorsWomanProsecutorsProsecutorsNineProsecutorsProsecutorsProsecutorsNineProsecutorsLProsecutorsLLProsecutorsAtProsecutorsLProsecutorsPoliceProsecutorsProsecutorsWhenProsecutorsLNineProsecutorsNineLProsecutorsProsecutorsLMarProsecutorsProsecutorsAtNineProsecutorsBarProsecutorsProsecutorsBarLBarLLNineLLLWhenLLWomanLLAtLProsecutorsManProsecutorsLAtProsecutorsLAfterProsecutorsProsecutorsAfterLLPoliceLProsecutorsNineWomanProsecutorsProsecutorsInProsecutorsProsecutorsFiveProsecutorsProsecutorsWomanBarProsecutorsLPoliceProsecutorsLWhenProsecutorsAtLNineNineProsecutorsWomenProsecutorsProsecutorsPoliceLLInProsecutorsLWomanProsecutorsLInLProsecutorsInLLFiveLLWomenLProsecutorsBarBarLProsecutorsAfterNineProsecutorsWomanLProsecutorsWhen\n"
     ]
    }
   ],
   "source": [
    "# # Sample text to summarize\n",
    "text = \"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\"\"\"\n",
    "\n",
    "# Encode the text into tokens\n",
    "inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "# , max_length=1024\n",
    "\n",
    "# Move the input tensors to the same device as the model\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Generate a summary of the encoded text\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    num_beams=4,\n",
    "    # max_length=51,\n",
    "    # early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dry19\\Desktop\\EECS595\\Final Project\\BART Model Set Up.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     output_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         inputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         num_beams\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39m# max_length=50,  # Optionally set a max length if desired\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Decode the generated tokens for each input in the batch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m output_texts \u001b[39m=\u001b[39m [\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     tokenizer\u001b[39m.\u001b[39mdecode(token, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m output_tokens\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m ]\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\transformers\\generation\\utils.py:1752\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1746\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1747\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1748\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1749\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1752\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   1753\u001b[0m         input_ids,\n\u001b[0;32m   1754\u001b[0m         beam_scorer,\n\u001b[0;32m   1755\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1756\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1757\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1758\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1759\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1760\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1761\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1762\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1763\u001b[0m     )\n\u001b[0;32m   1765\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[0;32m   1766\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\transformers\\generation\\utils.py:3091\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3087\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   3089\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 3091\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   3092\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   3093\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   3094\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   3095\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   3096\u001b[0m )\n\u001b[0;32m   3098\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3099\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1577\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1572\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1573\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1574\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1575\u001b[0m         )\n\u001b[1;32m-> 1577\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m   1578\u001b[0m     input_ids,\n\u001b[0;32m   1579\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1580\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1581\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[0;32m   1582\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1583\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1584\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1585\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1586\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1587\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1588\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1589\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1590\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1591\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1592\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1593\u001b[0m )\n\u001b[0;32m   1595\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\n\u001b[0;32m   1596\u001b[0m lm_logits \u001b[39m=\u001b[39m lm_logits \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\u001b[39m.\u001b[39mto(lm_logits\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1463\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1456\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1457\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[0;32m   1458\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1459\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1460\u001b[0m     )\n\u001b[0;32m   1462\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1463\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   1464\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1465\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1466\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m   1467\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1468\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1469\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1470\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1471\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1472\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1473\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1474\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1475\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1476\u001b[0m )\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1479\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\dry19\\Desktop\\EECS595\\Final Project\\BART Model Set Up.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=240'>241</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=241'>242</a>\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=242'>243</a>\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=252'>253</a>\u001b[0m         use_cache,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=253'>254</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=254'>255</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=255'>256</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=256'>257</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=257'>258</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=258'>259</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=259'>260</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=260'>261</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=261'>262</a>\u001b[0m             head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=262'>263</a>\u001b[0m         ),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=263'>264</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=264'>265</a>\u001b[0m             cross_attn_head_mask[idx]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m             \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m             \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m         ),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=269'>270</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=271'>272</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=272'>273</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=274'>275</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\dry19\\Desktop\\EECS595\\Final Project\\BART Model Set Up.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m fc1_concat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(hidden_states_reshaped, p1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# fc1_concat1 = torch.stack([fc1(hidden_states) for fc1 in self.fc1], dim=0)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# print(\"p1\", p1.shape)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# print(\"q1\", q1.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# print(\"k1\", k1.shape)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m mh_attn_weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmh_attn(query\u001b[39m=\u001b[39;49mhidden_states_reshaped, key\u001b[39m=\u001b[39;49mp1) \u001b[39m# shape: (num_experts, batch_size, num_heads, seq_len, embed_dim)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# print(\"attention_weight_1\", attention_weight_1.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# print(\"mh_attn_weight\", mh_attn_weight.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# print(\"Same:\", torch.allclose(attention_weight_1, mh_attn_weight, rtol=0.001))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m# print(torch.abs(attention_weight_1 - mh_attn_weight).mean())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# print(torch.abs(attention_weight_1 - mh_attn_weight).max())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# print()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m attention_weight_1_norm_expert \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     mh_attn_weight, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m )\u001b[39m.\u001b[39mmean(dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\EECS595project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\dry19\\Desktop\\EECS595\\Final Project\\BART Model Set Up.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_heads(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwk(key), batch_size, seq_len_k\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m )  \u001b[39m# (num_features, batch_size, num_heads, seq_len_k, depth)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Compute attention\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m attention_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_attention(query, key, seq_len_k)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mreturn\u001b[39;00m attention_weights\n",
      "\u001b[1;32mc:\\Users\\dry19\\Desktop\\EECS595\\Final Project\\BART Model Set Up.ipynb Cell 20\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mComputes the scaled dot-product attention.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mtorch.Tensor: Attention weights.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# Expand query to match key's features\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39m# query = query.unsqueeze(0).expand(\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39m#     self.num_features, -1, -1, -1, -1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m# )  # (num_features, batch_size, num_heads, seq_len_q, depth)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m# Perform batch matrix multiplication\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m matmul_qk \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     query, key\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m )  \u001b[39m# (num_features, batch_size, num_heads, seq_len_q, seq_len_k)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# Scale\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dry19/Desktop/EECS595/Final%20Project/BART%20Model%20Set%20Up.ipynb#X25sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m scale_factor \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare the batched input\n",
    "input_texts = [\n",
    "    \"Who is the president of China?\",\n",
    "    \"Who is the president of the US?\",\n",
    "    \"Who is the president of the Russia?\",\n",
    "    # \"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband. Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other. In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\"\"\",\n",
    "    # \"\"\"Your model's primary bottlenecks appear to be matrix multiplication and linear layer operations, both in terms of computation and possibly memory usage. Focusing your optimization efforts on these areas, along with minimizing unnecessary memory operations, could lead to significant improvements in performance. Remember, optimizations can sometimes affect model accuracy, so it's important to validate your model's performance after making any changes.\"\"\",\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    input_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    # truncation=True\n",
    ")\n",
    "\n",
    "# Move the input tensors to the same device as the model\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Generate the output\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        num_beams=4,\n",
    "        # max_length=50,  # Optionally set a max length if desired\n",
    "    )\n",
    "\n",
    "# Decode the generated tokens for each input in the batch\n",
    "output_texts = [\n",
    "    tokenizer.decode(token, skip_special_tokens=True) for token in output_tokens\n",
    "]\n",
    "\n",
    "# Print each output\n",
    "for output in output_texts:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "#              record_shapes=True) as prof:\n",
    "#     with record_function(\"model_inference\"):\n",
    "#         # Your model inference code here\n",
    "#         outputs = model.generate(\n",
    "#         inputs[\"input_ids\"],\n",
    "#         attention_mask=inputs[\"attention_mask\"],\n",
    "#         num_beams=4,\n",
    "#         # max_length=50,  # Optionally set a max length if desired\n",
    "#         )\n",
    "#         # model(input_data)\n",
    "\n",
    "# print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Mask import TextMaskingDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "# from Mask import pad_sequences\n",
    "# from Mask import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences):\n",
    "    input_ids = [ i['input_ids'].squeeze().tolist() for i in sequences]\n",
    "    attention_mask = [ i['attention_mask'].squeeze().tolist() for i in sequences]\n",
    "    padded = tokenizer.pad({\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }, return_tensors='pt')\n",
    "    \n",
    "    return padded\n",
    "\n",
    "def pad_indice(indice,max_len=1024):\n",
    "    if max_len > max(len(seq) for seq in indice):\n",
    "       max_len =  max(len(seq) for seq in indice)\n",
    "    padded_sequences = [seq + [False] * (max_len - len(seq)) for seq in indice]\n",
    "    return torch.tensor(padded_sequences)\n",
    "\n",
    "def collate(batch):\n",
    "    masked = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    indice = [item[2] for item in batch]\n",
    "    masked_pad = pad_sequences(masked)\n",
    "    target_pad = pad_sequences(target)\n",
    "    indice_pad = pad_indice(indice)\n",
    "    return masked_pad,target_pad,indice_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextMaskingDataset(file_path=\"ARC_Corpus.txt\",tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False,collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 2\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_epochs * len(dataloader)\n",
    "    )\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large international companies are involved in bauxite, iron ore, diamond, and gold mining operations.\n",
      "Paleoceanography, 8(2): 193-208.\n",
      "torch.Size([2, 26])\n",
      "cuda\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "tensor([[    0, 39012,   758,   451,    32,   963,    11,   741,  8624,  1459,\n",
      "          6440,  9717, 11720, 50264,  1637,  4481,  1414,  2540, 30254,     5,\n",
      "          3645,    30,  8794,     5, 11445,     2],\n",
      "        [    0,   510,  1627,   139, 35210, 10486,   290,  1640,   176,    43,\n",
      "         50264,  2540, 30254,     5,  3645,    30,  8794,     5, 11445,     2,\n",
      "             1,     1,     1,     1,     1,     1]], device='cuda:0')\n",
      "tensor(20.6542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Of course, for many in the media, “hydrogen sulphide delivery helps prevent disease damage in cells in certain disease models” will always be trumped by “farts cure cancer” when it comes to headlines.\n",
      "The same problems apply with wolf-domestic dog hybrids.\n",
      "torch.Size([2, 53])\n",
      "cuda\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[    0, 10643,   768,    13,   171,    11,     5,   433,    44,    48,\n",
      "         30420, 26759, 32494,  1949,  2996,  2607,  2097,  2199,  1880,    11,\n",
      "          4590,    11,  1402,  2199,  3092,    17,    46,    40,   460,    28,\n",
      "         20125,   196,    30,    44,    48,   506,  7870, 13306, 50264,    77,\n",
      "            24,   606,     7,  6337,  2540, 30254,     5,  3645,    30,  8794,\n",
      "             5, 11445,     2],\n",
      "        [    0,   133,   276,  1272,  3253,    19, 50264,  2335, 34297,  2540,\n",
      "         30254,     5,  3645,    30,  8794,     5, 11445,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]], device='cuda:0')\n",
      "tensor(32.8077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "taking stock of delightful days\n",
      "The an- tlu-opologist and the ethnologist find in trop- ical America some of the most complicated and interesting problems of research.\n",
      "torch.Size([2, 38])\n",
      "cuda\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[    0, 16883, 50264,     9, 24897,   360,  2540, 30254,     5,  3645,\n",
      "            30,  8794,     5, 11445,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   133,    41,    12,   326,  6487,    12,  1517,  6393,     8,\n",
      "             5, 35526,  6393,   465,    11, 50264,  1437,  3569,   730,   103,\n",
      "             9,     5,   144,  6336,     8,  2679,  1272,     9,   557,  2540,\n",
      "         30254,     5,  3645,    30,  8794,     5, 11445,     2]],\n",
      "       device='cuda:0')\n",
      "tensor(19.5359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ORDER ODONATA (Damselflies and Dragonflies) Diagnosis: large, to over three inches long; four wings, transparent and membranous, held vertically (damselflies) or laterally (dragonflies) at rest; chew- ing mouth parts, tooth-like; nymphs aquatic, feeding on mosquito larvae to small fish; adults terrestrial, feeding on other insects (Figure 14.27).\n",
      "until they institute such safeguards and assurances of chaste maidenhood as characterize Hebrew social life?\n",
      "torch.Size([2, 83])\n",
      "cuda\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[    0, 11200,  2076, 20778,  2191, 25912,    36, 33905, 13367, 15124,\n",
      "             8, 15522, 37085,    43, 23465, 13310,   739,     7,    81,   130,\n",
      "          4877,   251,   237, 11954,  8818,     8, 48153,   260,  1827,   547,\n",
      "         32039,    36, 15177, 13367, 15124,    43,    50,   423,  2368,    36,\n",
      "         46933, 37085,    43,    23,  1079, 34431,    12, 21691,  6085,  1667,\n",
      "         13495,    12,  3341, 50264, 31092, 10943,    15, 22443, 40046,     7,\n",
      "           650,  3539,  3362, 39056, 10943,    15,    97, 20289,    36, 40683,\n",
      "           501,     4,  2518,    43,  2540, 30254,     5,  3645,    30,  8794,\n",
      "             5, 11445,     2],\n",
      "        [    0, 29510,    51, 50264,   215, 25563,     8, 19820,     9,  1855,\n",
      "         14631, 17077,  8489,    25, 33776, 27428,   592,   301,   116,  2540,\n",
      "         30254,     5,  3645,    30,  8794,     5, 11445,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]], device='cuda:0')\n",
      "tensor(20.6022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Sex brought the variations that could allow organisms to survive change” (2000, p. 163).\n",
      "Homo erectus had a long tenure; the earliest Homo erectus fossils are dated to roughly 1.8 million years ago, while the youngest fossils assigned to this species date to roughly 300 thousand years ago (ka).\n",
      "torch.Size([2, 52])\n",
      "cuda\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[    0, 35581,  1146,     5, 18746,    14, 50264,  1157, 28340,     7,\n",
      "          6008,   464,    17,    46,    36, 17472,   181, 28158,    43,  2540,\n",
      "         30254,     5,  3645,    30,  8794,     5, 11445,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    0,   725, 13318, 24958,   687,    56,    10,   251,  7703,     5,\n",
      "         13342, 42769, 24958,   687, 34642,    32,  7000,     7,  3667,   112,\n",
      "             4,   398,   153,   107,   536, 50264,     5,  8733, 34642,  5530,\n",
      "             7,    42,  4707,  1248,     7,  3667,  2993,  7673,   107,   536,\n",
      "            36,  2348,    43,  2540, 30254,     5,  3645,    30,  8794,     5,\n",
      "         11445,     2]], device='cuda:0')\n",
      "tensor(24.0750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0: Loss 23.534988403320312\n",
      "Large international companies are involved in bauxite, iron ore, diamond, and gold mining operations.\n",
      "Paleoceanography, 8(2): 193-208.\n",
      "torch.Size([2, 26])\n",
      "cuda\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "tensor([[    0, 39012, 50264,   451,    32,   963,    11,   741,  8624,  1459,\n",
      "          6440,  9717, 11720,     8,  1637,  4481,  1414,  2540, 30254,     5,\n",
      "          3645,    30,  8794,     5, 11445,     2],\n",
      "        [    0,   510,  1627,   139, 35210, 10486,   290,  1640,   176,    43,\n",
      "         50264,  2540, 30254,     5,  3645,    30,  8794,     5, 11445,     2,\n",
      "             1,     1,     1,     1,     1,     1]], device='cuda:0')\n",
      "tensor(21.8881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Of course, for many in the media, “hydrogen sulphide delivery helps prevent disease damage in cells in certain disease models” will always be trumped by “farts cure cancer” when it comes to headlines.\n",
      "The same problems apply with wolf-domestic dog hybrids.\n",
      "torch.Size([2, 52])\n",
      "cuda\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[    0, 10643,   768,    13,   171,    11,     5,   433, 50264, 32494,\n",
      "          1949,  2996,  2607,  2097,  2199,  1880,    11,  4590,    11,  1402,\n",
      "          2199,  3092,    17,    46,    40,   460,    28, 20125,   196,    30,\n",
      "            44,    48,   506,  7870, 13306,  1668,    17,    46,    77,    24,\n",
      "           606,     7,  6337,  2540, 30254,     5,  3645,    30,  8794,     5,\n",
      "         11445,     2],\n",
      "        [    0,   133,   276,  1272,  3253, 50264, 23255,    12, 12623, 23862,\n",
      "          2335, 34297,  2540, 30254,     5,  3645,    30,  8794,     5, 11445,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]], device='cuda:0')\n",
      "tensor(19.1189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "taking stock of delightful days\n",
      "The an- tlu-opologist and the ethnologist find in trop- ical America some of the most complicated and interesting problems of research.\n",
      "torch.Size([2, 39])\n",
      "cuda\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[    0, 16883,   388,     9, 50264,   360,  2540, 30254,     5,  3645,\n",
      "            30,  8794,     5, 11445,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   133,    41,    12,   326,  6487,    12,  1517,  6393,     8,\n",
      "             5, 35526,  6393,   465, 50264, 37473,    12,  1437,  3569,   730,\n",
      "           103,     9,     5,   144,  6336,     8,  2679,  1272,     9,   557,\n",
      "          2540, 30254,     5,  3645,    30,  8794,     5, 11445,     2]],\n",
      "       device='cuda:0')\n",
      "tensor(15.4049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ORDER ODONATA (Damselflies and Dragonflies) Diagnosis: large, to over three inches long; four wings, transparent and membranous, held vertically (damselflies) or laterally (dragonflies) at rest; chew- ing mouth parts, tooth-like; nymphs aquatic, feeding on mosquito larvae to small fish; adults terrestrial, feeding on other insects (Figure 14.27).\n",
      "until they institute such safeguards and assurances of chaste maidenhood as characterize Hebrew social life?\n",
      "torch.Size([2, 83])\n",
      "cuda\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "tensor([[    0, 11200,  2076, 20778,  2191, 25912,    36, 33905, 13367, 15124,\n",
      "             8, 50264, 23465, 13310,   739,     7,    81,   130,  4877,   251,\n",
      "           237, 11954,  8818,     8, 48153,   260,  1827,   547, 32039,    36,\n",
      "         15177, 13367, 15124,    43,    50,   423,  2368,    36, 46933, 37085,\n",
      "            43,    23,  1079, 34431,    12, 21691,  6085,  1667, 13495,    12,\n",
      "          3341,   295, 36935,    29, 31092, 10943,    15, 22443, 40046,     7,\n",
      "           650,  3539,  3362, 39056, 10943,    15,    97, 20289,    36, 40683,\n",
      "           501,     4,  2518,    43,  2540, 30254,     5,  3645,    30,  8794,\n",
      "             5, 11445,     2],\n",
      "        [    0, 29510,    51, 14619,   215, 25563,     8, 19820,     9,  1855,\n",
      "         14631, 17077,  8489,    25, 33776, 27428, 50264,   301,   116,  2540,\n",
      "         30254,     5,  3645,    30,  8794,     5, 11445,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]], device='cuda:0')\n",
      "tensor(17.7103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Sex brought the variations that could allow organisms to survive change” (2000, p. 163).\n",
      "Homo erectus had a long tenure; the earliest Homo erectus fossils are dated to roughly 1.8 million years ago, while the youngest fossils assigned to this species date to roughly 300 thousand years ago (ka).\n",
      "torch.Size([2, 52])\n",
      "cuda\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[    0, 35581,  1146, 50264, 18746,    14,   115,  1157, 28340,     7,\n",
      "          6008,   464,    17,    46,    36, 17472,   181, 28158,    43,  2540,\n",
      "         30254,     5,  3645,    30,  8794,     5, 11445,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    0,   725, 13318, 24958,   687,    56,    10,   251,  7703,     5,\n",
      "         13342, 42769, 24958,   687, 34642,    32,  7000,     7,  3667,   112,\n",
      "             4,   398,   153,   107,   536,   150,     5,  8733, 34642,  5530,\n",
      "             7,    42,  4707,  1248,     7,  3667,  2993,  7673, 50264,   536,\n",
      "            36,  2348,    43,  2540, 30254,     5,  3645,    30,  8794,     5,\n",
      "         11445,     2]], device='cuda:0')\n",
      "tensor(21.0289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1: Loss 19.030234146118165\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for masked_sentences, original_sentences, masked_indices in dataloader:\n",
    "        \n",
    "        print(masked_sentences[\"attention_mask\"].shape)\n",
    "        print(device)\n",
    "    \n",
    "        att_mask= masked_sentences[\"attention_mask\"].to(device)\n",
    "        input_id=masked_sentences[\"input_ids\"].to(device)\n",
    "        label = original_sentences[\"attention_mask\"].to(device)\n",
    "        print(att_mask)\n",
    "        print(input_id)\n",
    "        output = model(\n",
    "                        input_id,\n",
    "                        att_mask,\n",
    "                        labels = label\n",
    "        )\n",
    "\n",
    "\n",
    "        # original_sentences = {k: v.to(device) for k, v in original_sentences.items()}\n",
    "        # target_tokens = model.generate(\n",
    "        #                 original_sentences[\"input_ids\"],\n",
    "        #                 attention_mask=original_sentences[\"attention_mask\"],\n",
    "        # )\n",
    "        \n",
    "        # masked_indices = masked_indices.view(-1) == 1\n",
    "        # outputs = output_tokens.view(-1, outputs.size(-1))\n",
    "        # targets = target_tokens.view(-1)\n",
    "\n",
    "        # # Select only the outputs and targets at the masked positions\n",
    "        # masked_outputs = outputs[masked_indices]\n",
    "        # masked_targets = targets[masked_indices]\n",
    "\n",
    "        # Compute loss only on masked tokens\n",
    "        # loss = loss_fn(masked_outputs, masked_targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss = output.loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"path/to/save/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs595python10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
