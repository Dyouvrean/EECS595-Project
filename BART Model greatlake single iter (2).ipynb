{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U  torch transformers torchsummary  \n",
    "# apex jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
    "from transformers.file_utils import (\n",
    "    add_code_sample_docstrings,\n",
    "    add_end_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    "    Seq2SeqQuestionAnsweringModelOutput,\n",
    "    Seq2SeqSequenceClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.models.bart.modeling_bart import (\n",
    "    BartLearnedPositionalEmbedding,\n",
    "    BartDecoderLayer,\n",
    "    BartPreTrainedModel,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_attn_mask_utils import (\n",
    "    _prepare_4d_attention_mask,\n",
    "    _prepare_4d_causal_attention_mask,\n",
    ")\n",
    "from transformers.activations import ACT2FN\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART Model Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedFeatureSpecificMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads, d_model, num_features):\n",
    "        \"\"\"\n",
    "        Initializes the VectorizedFeatureSpecificMultiHeadAttention module.\n",
    "\n",
    "        Parameters:\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_model (int): Dimensionality of the input feature space.\n",
    "        num_features (int): Number of distinct features / experts.\n",
    "        \"\"\"\n",
    "        super(VectorizedFeatureSpecificMultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_features = num_features\n",
    "\n",
    "        assert d_model % self.num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(\n",
    "            d_model, d_model\n",
    "        )  # Same as original since key is already feature-specific\n",
    "\n",
    "    def split_heads(self, x, batch_size, seq_len):\n",
    "        \"\"\"\n",
    "        Splits the last dimension of x into (num_heads, depth) and reshapes.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        seq_len (int): Sequence length for the reshaping process.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Reshaped tensor.\n",
    "        \"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.num_heads, self.depth)\n",
    "        x = x.reshape(*new_shape).permute(\n",
    "            0, 1, 3, 2, 4\n",
    "        )  # (num_features, batch_size, num_heads, seq_len, depth)\n",
    "        return x\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        \"\"\"\n",
    "        Forward pass for the VectorizedFeatureSpecificMultiHeadAttention.\n",
    "\n",
    "        Parameters:\n",
    "        query (torch.Tensor): Query tensor of shape (batch_size, seq_len_q, d_model).\n",
    "        key (torch.Tensor): Key tensor of shape (num_features, batch_size, seq_len_k, d_model).\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Concatenated attention weights across all heads and features.\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_q = query.size(1)\n",
    "        seq_len_k = key.size(2)\n",
    "\n",
    "        # Prepare query and key\n",
    "        query = self.split_heads(\n",
    "            self.wq(query), batch_size, seq_len_q\n",
    "        )  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        key = self.split_heads(\n",
    "            self.wk(key), batch_size, seq_len_k\n",
    "        )  # (num_features, batch_size, num_heads, seq_len_k, depth)\n",
    "\n",
    "        # Compute attention\n",
    "        attention_weights = self.compute_attention(query, key, seq_len_k)\n",
    "\n",
    "        return attention_weights\n",
    "\n",
    "    def compute_attention(self, query, key, seq_len_k):\n",
    "        \"\"\"\n",
    "        Computes the scaled dot-product attention.\n",
    "\n",
    "        Parameters:\n",
    "        query (torch.Tensor): Query tensor.\n",
    "        key (torch.Tensor): Key tensor.\n",
    "        seq_len_k (int): Key sequence length.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Attention weights.\n",
    "        \"\"\"\n",
    "        # Expand query to match key's features\n",
    "        # query = query.unsqueeze(0).expand(\n",
    "        #     self.num_features, -1, -1, -1, -1\n",
    "        # )  # (num_features, batch_size, num_heads, seq_len_q, depth)\n",
    "\n",
    "        # Perform batch matrix multiplication\n",
    "        matmul_qk = torch.matmul(\n",
    "            query, key.transpose(-2, -1)\n",
    "        )  # (num_features, batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Scale\n",
    "        scale_factor = 1 / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n",
    "        attention_weights = matmul_qk * scale_factor\n",
    "\n",
    "        # # Apply softmax along the dimension of key's sequence length\n",
    "        # attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "\n",
    "        # Reshape to combine features and batch\n",
    "        # _, _, num_heads, _, _ = scaled_attention_logits.shape\n",
    "        # attention_weights = scaled_attention_logits.permute(\n",
    "        #     1, 2, 0, 3, 4\n",
    "        # ).contiguous()  # (batch_size, num_heads, num_features, seq_len_q, seq_len_k)\n",
    "\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BartDecoderLayer Modification\n",
    "class CustomBartDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: BartConfig, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.num_replicas = num_replicas\n",
    "        self.embed_dim = config.d_model  # Assuming embed_dim is d_model\n",
    "        self.fc1 = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "                for _ in range(num_replicas)\n",
    "            ]\n",
    "        )\n",
    "        # print(self.embed_dim, config.decoder_ffn_dim)\n",
    "        # self.fc2 = nn.ModuleList(\n",
    "        #     [nn.Linear(self.embed_dim, self.embed_dim, bias=False) for _ in range(num_replicas)]\n",
    "        # )\n",
    "        self.q1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.attention_weight_bias = nn.Parameter(torch.zeros(num_replicas, 1, 1, 1))\n",
    "\n",
    "        # self.q2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.k2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        # print(self.embed_dim, config.decoder_ffn_dim, config.decoder_attention_heads)\n",
    "\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "        self.mh_attn = VectorizedFeatureSpecificMultiHeadAttention(\n",
    "            num_heads=config.decoder_attention_heads,\n",
    "            d_model=self.embed_dim,\n",
    "            num_features=self.num_replicas,\n",
    "        )\n",
    "        # config.decoder_attention_heads\n",
    "        # for i, fc1 in enumerate(self.fc1):\n",
    "        #     # Initialize with a simple pattern, e.g., all elements in the weight matrix are set to the index of the layer\n",
    "        #     nn.init.constant_(fc1.weight, 17 * i)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        outputs = self.layer(x, *args, **kwargs)\n",
    "        hidden_states = outputs[0]\n",
    "        residual = hidden_states\n",
    "\n",
    "        batch_size = hidden_states.size(0)\n",
    "        seq_len = hidden_states.size(1)\n",
    "\n",
    "        p1 = torch.stack([fc1.weight for fc1 in self.fc1], dim=0)\n",
    "        # .transpose(0, 1)\n",
    "        p1 = p1.unsqueeze(1).expand(-1, batch_size, -1, -1)\n",
    "\n",
    "        hidden_states_reshaped = hidden_states.unsqueeze(0).expand(\n",
    "            self.num_replicas, -1, -1, -1\n",
    "        )\n",
    "        fc1_concat = torch.matmul(hidden_states_reshaped, p1)\n",
    "        # fc1_concat1 = torch.stack([fc1(hidden_states) for fc1 in self.fc1], dim=0)\n",
    "\n",
    "        # print(\"p1\", p1.shape)\n",
    "        # print(\"hidden_states_reshaped\",hidden_states_reshaped.shape)\n",
    "        # print(\"fc1_concat1\",fc1_concat1.shape)\n",
    "        # print(\"fc1_concat\", fc1_concat.shape)\n",
    "        # torch.set_printoptions(sci_mode=False)\n",
    "        # print(\"Same:\", torch.allclose(fc1_concat, fc1_concat1, rtol = 0.001))\n",
    "        # print(torch.abs(fc1_concat - fc1_concat1).mean())\n",
    "        # print(torch.abs(fc1_concat - fc1_concat1).max())\n",
    "        # print()\n",
    "\n",
    "        # q1 = self.q1(hidden_states).unsqueeze(0).expand(self.num_replicas, -1, -1, -1)\n",
    "        # k1 = self.k1(p1)\n",
    "        # attention_weight_1 = scaled_dot_product_attention(q1, k1, self.embed_dim)\n",
    "        # attention_weight_1 = attention_weight_1 + self.attention_weight_bias\n",
    "\n",
    "        # print(\"q1\", q1.shape)\n",
    "        # print(\"k1\", k1.shape)\n",
    "        mh_attn_weight = self.mh_attn(\n",
    "            query=hidden_states_reshaped, key=p1\n",
    "        )  # shape: (num_experts, batch_size, num_heads, seq_len, embed_dim)\n",
    "        # print(\"attention_weight_1\", attention_weight_1.shape)\n",
    "        # print(\"mh_attn_weight\", mh_attn_weight.shape)\n",
    "        # print(\"Same:\", torch.allclose(attention_weight_1, mh_attn_weight, rtol=0.001))\n",
    "        # print(torch.abs(attention_weight_1 - mh_attn_weight).mean())\n",
    "        # print(torch.abs(attention_weight_1 - mh_attn_weight).max())\n",
    "        # print()\n",
    "\n",
    "        # attention_weight_1_norm_expert = nn.functional.softmax(\n",
    "        #     mh_attn_weight, dim=0\n",
    "        # ).mean(dim = 2)\n",
    "        # attention_weight_1_norm_feature = nn.functional.softmax(\n",
    "        #     mh_attn_weight, dim=-1\n",
    "        # ).mean(dim = 2)\n",
    "        # mh_attn_weight = (\n",
    "        #     3 * attention_weight_1_norm_expert + attention_weight_1_norm_feature\n",
    "        # )\n",
    "        mh_attn_weight = mh_attn_weight.mean(dim=2)\n",
    "        # print(self.num_replicas)\n",
    "        # print(\"q1\", q1.shape)\n",
    "        # print(\"k1\", k1.shape)\n",
    "        # print(\"fc1_concat\", fc1_concat.shape)\n",
    "        # print(\"attention_weight_1\", attention_weight_1.shape)\n",
    "        fc1_weighted = fc1_concat * mh_attn_weight\n",
    "        hidden_states = fc1_weighted.mean(dim=0)\n",
    "        # self.activation_fn\n",
    "        # print((attention_weight_1.sum(dim=(1,2,3))))\n",
    "        # hidden_states = nn.functional.dropout(\n",
    "        #     hidden_states, p=self.activation_dropout, training=self.training\n",
    "        # )\n",
    "\n",
    "        # # Vectorized operation for fc2 layers\n",
    "        # q2 = self.q2(hidden_states)\n",
    "        # fc2_concat = torch.stack([fc2(hidden_states) for fc2 in self.fc2], dim=0)\n",
    "        # p2 = torch.stack([fc2.weight.data for fc2 in self.fc2], dim=0)\n",
    "        # p2 = p2.unsqueeze(2).expand(-1, -1, batch_size, -1)\n",
    "        # k2 = self.k2(p2)\n",
    "        # attention_weight_2 = scaled_dot_product_attention(q2, k2, self.embed_dim)\n",
    "        # fc2_weighted = fc2_concat * attention_weight_2\n",
    "        # hidden_states = fc2_weighted.mean(dim=0)\n",
    "\n",
    "        hidden_states = nn.functional.dropout(\n",
    "            hidden_states, p=self.dropout, training=self.training\n",
    "        )\n",
    "\n",
    "        hidden_states = hidden_states + residual\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        return (hidden_states,) + outputs[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-10): 11 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CustomBartDecoderLayer(\n",
       "          (layer): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (fc1): ModuleList(\n",
       "            (0-2): 3 x Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (q1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (mh_attn): VectorizedFeatureSpecificMultiHeadAttention(\n",
       "            (wq): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (wk): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained BART model\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "config = BartConfig.from_pretrained(model_name)\n",
    "# print(config)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model_save_path = \"./BART_model\"\n",
    "# config = BartConfig.from_pretrained(model_save_path)\n",
    "# # print(config)\n",
    "# model = BartForConditionalGeneration.from_pretrained(model_save_path, config=config)\n",
    "# tokenizer = BartTokenizer.from_pretrained(model_save_path)\n",
    "\n",
    "\n",
    "# Check if CUDA GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_replicas = 3\n",
    "\n",
    "# Replace all customized layers\n",
    "for i, layer in enumerate(model.model.decoder.layers):    \n",
    "    if i == len(model.model.decoder.layers) - 1:\n",
    "        model.model.decoder.layers[i] = CustomBartDecoderLayer(model.config, layer)\n",
    "\n",
    "model_weights_path = r\"./model_weights/facebook/bart-large-cnn.pth\"\n",
    "#model_weights_path = r\"./model_weights/facebook/bart-large-cnn2.pth\"\n",
    "# # Load model weights to the device\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load(model_weights_path, map_location=\"cuda\"))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_weights_path, map_location=\"cpu\"))\n",
    "\n",
    "# Move the model to the specified device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Warehouse\n",
    "# model_weigts_path = r\"./model_weights/facebook/bart-large-cnn2.pth\"\n",
    "# model = BartForConditionalGeneration(config=config)\n",
    "\n",
    "# for i, layer in enumerate(model.model.encoder.layers):\n",
    "#     model.model.encoder.layers[i] = CustomBartEncoderLayer(model.config, layer)\n",
    "\n",
    "\n",
    "# #save pretrained model weights\n",
    "# torch.save(model.state_dict(), model_weigts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 413636611\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze pretrained weight\n",
    "\n",
    "# Step 1: Freeze all pretrained weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Step 2: Unfreeze the weights in custom layers\n",
    "for i in range(len(model.model.decoder.layers)):\n",
    "    layer = model.model.decoder.layers[i]\n",
    "    if i == len(model.model.decoder.layers) - 1 and isinstance(layer, CustomBartDecoderLayer):\n",
    "#     if isinstance(layer, CustomBartDecoderLayer):\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if isinstance(layer, BartDecoderLayer):\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "#     if i == len(model.model.decoder.layers) - 1:\n",
    "#         for param in layer.parameters():\n",
    "#             param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference / Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Sample text to summarize\n",
    "# text = \"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "# A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "# Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "# In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "# Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "# 2010 marriage license application, according to court documents.\n",
    "# Prosecutors said the marriages were part of an immigration scam.\n",
    "# On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "# After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "# Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "# All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "# Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "# Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "# The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "# Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "# Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "# If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\"\"\"\n",
    "\n",
    "# # Encode the text into tokens\n",
    "# inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "# # , max_length=1024\n",
    "\n",
    "# # Move the input tensors to the same device as the model\n",
    "# inputs = inputs.to(device)\n",
    "\n",
    "# # Generate a summary of the encoded text\n",
    "# summary_ids = model.generate(\n",
    "#     inputs[\"input_ids\"],\n",
    "#     num_beams=4,\n",
    "#     # max_length=51,\n",
    "#     # early_stopping=True\n",
    "# )\n",
    "\n",
    "# # Decode the summary\n",
    "# summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the president of China? China is home to one third of the world’s Aussen Aussen Ministerial staff as it is also home to 40% of the nation’’. China is also seat of one of the largest military bases in the world.\n",
      "Who is the president of the US? The correct choice is George W. Bush as it is conflicting with C which is who is really in charge of US foreign policy. The war is raging as both sides are competing for control of the affairs of the U.S.\n",
      "Who is the president of Russia? The correct choice is C which is Russian president is Vladimir Putin as it is conflicting with C. Dmitry Medvedev is also president of Ukraine as he is heir to Russian empire as well as president of the Soviet Union. Putin is also head of Russia's Communist Party.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the batched input\n",
    "input_texts = [\n",
    "    \"Who is the president of China?\",\n",
    "    \"Who is the president of the US?\",\n",
    "    \"Who is the president of Russia?\",\n",
    "    # \"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband. Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other. In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\"\"\",\n",
    "    # \"\"\"Your model's primary bottlenecks appear to be matrix multiplication and linear layer operations, both in terms of computation and possibly memory usage. Focusing your optimization efforts on these areas, along with minimizing unnecessary memory operations, could lead to significant improvements in performance. Remember, optimizations can sometimes affect model accuracy, so it's important to validate your model's performance after making any changes.\"\"\",\n",
    "    # \"\"\"Which technology was developed most recently? Options:\n",
    "    # A. television\n",
    "    # B. refrigerator\n",
    "    # C. cellular telephone\n",
    "    # D. airplane\n",
    "    # Which is the correct answer?\"\"\",\n",
    "]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    input_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    # truncation=True\n",
    ")\n",
    "\n",
    "# Move the input tensors to the same device as the model\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Generate the output\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        num_beams=4,\n",
    "        # max_length=20,  # Optionally set a max length if desired\n",
    "    )\n",
    "\n",
    "# Decode the generated tokens for each input in the batch\n",
    "output_texts = [\n",
    "    tokenizer.decode(token, skip_special_tokens=True) for token in output_tokens\n",
    "]\n",
    "\n",
    "# Print each output\n",
    "for output in output_texts:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "#              record_shapes=True) as prof:\n",
    "#     with record_function(\"model_inference\"):\n",
    "#         # Your model inference code here\n",
    "#         outputs = model.generate(\n",
    "#         inputs[\"input_ids\"],\n",
    "#         attention_mask=inputs[\"attention_mask\"],\n",
    "#         num_beams=4,\n",
    "#         # max_length=50,  # Optionally set a max length if desired\n",
    "#         )\n",
    "#         # model(input_data)\n",
    "\n",
    "# print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import random\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class PretrainedDataset(Dataset):\n",
    "#     def __init__(self, csv_file_paths, tokenizer, mask_probability=0.2, delete_probability=0.2):\n",
    "#         if not isinstance(csv_file_paths, list):\n",
    "#             raise TypeError(\"csv_file_paths should be a list of file paths.\")\n",
    "\n",
    "#         # Combine CSV files into one dataframe\n",
    "#         self.data  = pd.concat(\n",
    "#             [pd.read_csv(file_path, header=None) for file_path in csv_file_paths], \n",
    "#             ignore_index=True\n",
    "#         )\n",
    "#         self.data = self.data.values.squeeze(1)\n",
    "        \n",
    "# #         self.data = pd.read_csv(csv_file_path, header=None).iloc(:, 0).values.squeeze(1)\n",
    "# #         [:100]\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.mask_probability = mask_probability\n",
    "#         self.delete_probability = delete_probability\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def mask_text(self, text):\n",
    "#         tokens = text.split()\n",
    "#         return ' '.join([token if random.random() > self.mask_probability else '[MASK]' for token in tokens])\n",
    "\n",
    "#     def delete_end_text(self, text):\n",
    "#         tokens = text.split()\n",
    "#         cut_off = int(len(tokens) * random.uniform(0.7, 0.9))\n",
    "#         return ' '.join(tokens[:cut_off])\n",
    "\n",
    "#     def shuffle_text(self, text):\n",
    "#         tokens = text.split()\n",
    "#         random.shuffle(tokens)\n",
    "#         return ' '.join(tokens)\n",
    "\n",
    "#     def delete_random_text(self, text):\n",
    "#         tokens = text.split()\n",
    "#         return ' '.join([token if random.random() > self.delete_probability else '' for token in tokens])\n",
    "\n",
    "#     def corrupt_text(self, text):\n",
    "#         corruption_methods = [self.mask_text, self.delete_end_text, self.shuffle_text, self.delete_random_text]\n",
    "#         corruption_method = random.choice(corruption_methods)\n",
    "#         return corruption_method(text)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.data[idx]\n",
    "#         corrupted_text = \"Please reconstruct the original text: \" + self.corrupt_text(text)\n",
    "\n",
    "#         # Tokenize both original and corrupted text\n",
    "#         encoding = self.tokenizer(text, truncation=True, padding='max_length', return_tensors='pt')\n",
    "#         corrupted_encoding = self.tokenizer(corrupted_text, truncation=True, padding='max_length', return_tensors='pt')\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': corrupted_encoding['input_ids'].squeeze(0),\n",
    "#             'attention_mask': corrupted_encoding['attention_mask'].squeeze(0),\n",
    "#             'labels': encoding['input_ids'].squeeze(0)\n",
    "#         }\n",
    "    \n",
    "# train_paths = [\n",
    "#     r\"../data/1k ARC Corpus.csv\",\n",
    "#     r\"../data/fillered2.csv\",     \n",
    "#     r\"../data/1.5k_data1.csv\",    \n",
    "#     r\"../data/1.5k_data2.csv\",   \n",
    "#     r\"../data/wiki_1.csv\"\n",
    "# ]\n",
    "# val_paths = [  \n",
    "#     r\"../data/1.5k_data3.csv\",    \n",
    "# ]\n",
    "# train_ds = PretrainedDataset(train_paths, tokenizer)\n",
    "# val_ds = PretrainedDataset(val_paths, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneDataset(Dataset):\n",
    "    def __init__(self, csv_file_paths, tokenizer):\n",
    "        # Ensure csv_file_paths is a list\n",
    "        if not isinstance(csv_file_paths, list):\n",
    "            raise TypeError(\"csv_file_paths should be a list of file paths.\")\n",
    "\n",
    "        # Combine CSV files into one dataframe\n",
    "        self.dataframe = pd.concat(\n",
    "            [pd.read_csv(file_path) for file_path in csv_file_paths], \n",
    "            ignore_index=True\n",
    "        )\n",
    "#         .iloc[:10, :]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question_choices = self.dataframe.iloc[idx, 0]\n",
    "        answer = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "        # Tokenize without padding\n",
    "        encoding = self.tokenizer(\n",
    "            question_choices,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        label_encoding = self.tokenizer(\n",
    "            answer, truncation=True, return_tensors=\"pt\", add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": label_encoding[\"input_ids\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "csv_file_paths = [\n",
    "    r\"../data/train_df-arc_challenge.csv\",\n",
    "    r\"../data/train_df-ARC-Easy.csv\",\n",
    "    r\"../data/train_df_common.csv\",\n",
    "    r\"../data/train_df-TRIP.csv\",\n",
    "]\n",
    "\n",
    "val_csv_file_paths = [\n",
    "     r\"../data/valid_df-CE.csv\",\n",
    "     r\"../data/valid_df-TRIP.csv\",\n",
    "     r\"../data/valid_df-arc_challenge.csv\",\n",
    "     r\"../data/valid_df-ARC-Easy.csv\"\n",
    " ]\n",
    "\n",
    "train_ds = FinetuneDataset(csv_file_paths, tokenizer)\n",
    "\n",
    "val_ds = FinetuneDataset(val_csv_file_paths, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv_file_paths = [\n",
    "#      r\"../data/valid_df-CE.csv\",\n",
    "#      r\"../data/valid_df-TRIP.csv\",\n",
    "     r\"../data/valid_df-arc_challenge.csv\",\n",
    "#      r\"../data/valid_df-ARC-Easy.csv\"\n",
    " ]\n",
    "\n",
    "test_ds =FinetuneDataset(test_csv_file_paths, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_padding_collate_fn(batch):\n",
    "    max_length = max(\n",
    "        max([len(sample[\"input_ids\"]) for sample in batch]),\n",
    "        max([len(sample[\"labels\"]) for sample in batch]),\n",
    "    )\n",
    "\n",
    "    padded_batch = {\n",
    "        \"input_ids\": torch.stack(\n",
    "            [\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        sample[\"input_ids\"],\n",
    "                        torch.zeros(\n",
    "                            max_length - len(sample[\"input_ids\"]), dtype=torch.long\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "                for sample in batch\n",
    "            ]\n",
    "        ),\n",
    "        \"attention_mask\": torch.stack(\n",
    "            [\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        sample[\"attention_mask\"],\n",
    "                        torch.zeros(\n",
    "                            max_length - len(sample[\"attention_mask\"]), dtype=torch.long\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "                for sample in batch\n",
    "            ]\n",
    "        ),\n",
    "        \"labels\": torch.stack(\n",
    "            [\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        sample[\"labels\"],\n",
    "                        torch.zeros(\n",
    "                            max_length - len(sample[\"labels\"]), dtype=torch.long\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "                for sample in batch\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "bs = 8 # finetune dataset\n",
    "# bs = 6 # finetune dataset\n",
    "# bs = 3 # pretrain dataset\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds, batch_size=bs, shuffle=True, collate_fn=dynamic_padding_collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_ds, batch_size=bs, shuffle=True, collate_fn=dynamic_padding_collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader( test_ds, batch_size=bs, shuffle=True, collate_fn=dynamic_padding_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load a single batch from the DataLoader\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_dataloader)\n\u001b[0;32m----> 3\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extracting data from the batch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mFinetuneDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Tokenize without padding\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion_choices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m label_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m     30\u001b[0m     answer, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: label_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     37\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2856\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2859\u001b[0m     )\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2862\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2863\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2865\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# Load a single batch from the DataLoader\n",
    "data_iter = iter(train_dataloader)\n",
    "batch = next(data_iter)\n",
    "\n",
    "# Extracting data from the batch\n",
    "input_ids = batch[\"input_ids\"]\n",
    "attention_masks = batch[\"attention_mask\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "# Decode the token IDs back to text for a human-readable format\n",
    "decoded_inputs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "decoded_labels = [tokenizer.decode(ids, skip_special_tokens=True) for ids in labels]\n",
    "\n",
    "# Display the information\n",
    "print(\"Batch Content:\\n\")\n",
    "for i in range(len(decoded_inputs)):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Input:\", decoded_inputs[i])\n",
    "    print(\"Label:\", decoded_labels[i])\n",
    "    # print(\"Input IDs:\", input_ids[i])\n",
    "    # print(\"Attention Mask:\", attention_masks[i])\n",
    "    # print(\"Label IDs:\", labels[i])\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 46068.0 MB\n",
      "Used GPU Memory: 2602.0 MB\n",
      "Free GPU Memory: 42810.0 MB\n",
      "Total GPU Memory: 45413.12 MB\n",
      "Used GPU Memory: 2602.88 MB\n",
      "Free GPU Memory: 42810.25 MB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import re\n",
    "\n",
    "try:\n",
    "    nvidia_smi_output = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.total,memory.used,memory.free', '--format=csv,nounits,noheader']).decode('utf-8')\n",
    "    \n",
    "    # Parsing the output\n",
    "    memory_info = nvidia_smi_output.strip().split('\\n')[0].split(',')\n",
    "    total_memory, used_memory, free_memory = [float(x) for x in memory_info]\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory} MB\")\n",
    "    print(f\"Used GPU Memory: {used_memory} MB\")\n",
    "    print(f\"Free GPU Memory: {free_memory} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to run nvidia-smi:\", e)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()  # Clear cache for a better measure of free memory\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 2)\n",
    "    free_memory = torch.cuda.mem_get_info()[0] / (1024 ** 2)  # Returns (free, total) memory\n",
    "    used_memory = total_memory - free_memory\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} MB\")\n",
    "    print(f\"Used GPU Memory: {used_memory:.2f} MB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} MB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "# #optimizer = torch.optim.AdamW(model.parameters(), lr=7e-4)\n",
    "# # optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "# num_epochs = 10\n",
    "\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     name=\"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0.07*num_epochs * len(train_dataloader),\n",
    "#     num_training_steps=num_epochs * len(train_dataloader)\n",
    "# )\n",
    "\n",
    "# model.train()\n",
    "# min_epoch_loss = 1\n",
    "# # tqdm\n",
    "# for epoch in (range(num_epochs)):  # Define num_epochs\n",
    "#     epoch_loss = 0\n",
    "#     epoch_loss_val=0\n",
    "#     progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "#     for batch in progress_bar:\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         # # Debugging: Print shapes\n",
    "#         # print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "#         # print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        \n",
    "#         # # Convert model outputs to token IDs\n",
    "#         # pred_token_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "#         # # Decode labels and outputs\n",
    "#         # decoded_labels = [\n",
    "#         #     tokenizer.decode(ids, skip_special_tokens=True) for ids in labels\n",
    "#         # ]\n",
    "#         # decoded_outputs = [\n",
    "#         #     tokenizer.decode(ids, skip_special_tokens=True) for ids in pred_token_ids\n",
    "#         # ]\n",
    "#         # Debugging: Print decoded texts for verification\n",
    "# #         for i in range(\n",
    "# #             min(len(decoded_labels), 5)\n",
    "# #         ):  # Adjust number of examples to print\n",
    "# #             print(f\"Example {i+1}\")\n",
    "# #             print(\"Label:\", decoded_labels[i])\n",
    "# #             print(\"Output:\", decoded_outputs[i])\n",
    "# #             print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "#         loss = outputs.loss\n",
    "#         epoch_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step() \n",
    "#         progress_bar.set_postfix(batch_loss=f\"{(loss.item()/bs):.4f}\")\n",
    "            \n",
    "#     progress_bar_val = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "#     for batch in progress_bar_val:\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "\n",
    "#         loss_val = outputs.loss\n",
    "#         epoch_loss_val += loss.item()\n",
    "#         loss_val.backward()\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step() \n",
    "#         progress_bar_val.set_postfix(batch_loss=f\"{(loss.item()/bs):.4f}\")       \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#     epoch_loss_val /= len(val_dataloader)        \n",
    "#     epoch_loss /= len(train_dataloader)\n",
    "#     model_saving = \"\"\n",
    "# #     if epoch_loss < min_epoch_loss:\n",
    "# #         min_epoch_loss = epoch_loss\n",
    "# #         torch.save(model.state_dict(), model_weights_path)\n",
    "# #         model_saving = \"Model has been saved\"    \n",
    "#     if epoch_loss_val < min_epoch_loss:\n",
    "#         min_epoch_loss = epoch_loss_val\n",
    "#         torch.save(model.state_dict(), model_weights_path)\n",
    "#         model_saving = \"Model has been saved\" \n",
    "#     print(f\"train Epoch {epoch+1} loss: {epoch_loss:.4f}. {model_saving}\")\n",
    "#     print(f\"validation Epoch {epoch+1} loss: {epoch_loss_val:.4f}. {model_saving}\")\n",
    "\n",
    "# # 06:08\n",
    "# # 0.1781\n",
    "# # 0.0317\n",
    "\n",
    "# # 0.1813\n",
    "# # 0.1742\n",
    "# # 0.1227\n",
    "# # 12:00\n",
    "# # 0.0118\n",
    "\n",
    "# #pretrain:0.3316\n",
    "# #finetune:0.0010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:  # Consider using a validation dataloader\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=3,  # Beam search with 2 beams\n",
    "#                 min_length=1,  # Set a min length if desired\n",
    "#                 max_length=2,  # Set a max length if desired\n",
    "                # Add more generation parameters if needed\n",
    "            )\n",
    "    \n",
    "        input_ques = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "        decoded_labels = [tokenizer.decode(ids, skip_special_tokens=True) for ids in labels]\n",
    "        decoded_outputs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n",
    "\n",
    "        # Display decoded texts for verification (for the first example in the batch)\n",
    "        for i in range(bs):\n",
    "            print(\"Input: \", input_ques[i])\n",
    "            print(\"Label: \", decoded_labels[i])\n",
    "            print(\"Output: \", decoded_outputs[i])\n",
    "            print()\n",
    "        break\n",
    "# \n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save pretrained model weights\n",
    "# torch.save(model.state_dict(), model_weights_path)\n",
    "# print(f\"Model and tokenizer have been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv_file_paths = [\n",
    "     r\"../data/valid_df-CE.csv\",\n",
    "#        r\"../data/valid_df-TRIP.csv\",\n",
    "#     r\"../data/valid_df-arc_challenge.csv\",\n",
    "#       r\"../data/valid_df-ARC-Easy.csv\"\n",
    " ]\n",
    "\n",
    "test_ds =FinetuneDataset(test_csv_file_paths, tokenizer)\n",
    "test_dataloader = DataLoader( test_ds, batch_size=bs, shuffle=True, collate_fn=dynamic_padding_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "f1_metric_ma = evaluate.load(\"f1\")\n",
    "f1_metric_mi = evaluate.load(\"f1\")\n",
    "f1_metric_no = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load('recall')\n",
    "\n",
    "for batch in test_dataloader:  # Consider using a validation dataloader\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=3,  # Beam search with 2 beams\n",
    "#                 min_length=1,  # Set a min length if desired\n",
    "#                 max_length=2,  # Set a max length if desired\n",
    "                # Add more generation parameters if needed\n",
    "            )\n",
    "    \n",
    "    input_ques = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "    decoded_labels = [ord(tokenizer.decode(ids, skip_special_tokens=True).split()[4])-65 for ids in labels]\n",
    "\n",
    "    \n",
    "    decoded_outputs = [ord(tokenizer.decode(ids, skip_special_tokens=True).split()[4])-65 for ids in outputs]\n",
    "    \n",
    "    references = decoded_labels\n",
    "    predictions = decoded_outputs\n",
    "    metric_acc.add_batch(predictions=predictions, references=references)\n",
    "    f1_metric_ma.add_batch(predictions=predictions, references=references)\n",
    "    f1_metric_mi.add_batch(predictions=predictions, references=references)\n",
    "    f1_metric_no.add_batch(predictions=predictions, references=references)\n",
    "    precision_metric.add_batch(predictions=predictions, references=references)\n",
    "    recall_metric.add_batch(predictions=predictions, references=references)\n",
    "    \n",
    "    \n",
    "    \n",
    "score_acc = metric_acc.compute()\n",
    "print(score_acc)\n",
    "f1_macro = f1_metric_ma.compute(average=\"macro\")\n",
    "print(f1_macro)\n",
    "\n",
    "f1_micro = f1_metric_mi.compute(average=\"micro\")\n",
    "print(f1_macro)\n",
    "\n",
    "f1_None = f1_metric_no.compute(average=None)\n",
    "print(f1_macro)\n",
    "\n",
    "precision = precision_metric.compute(average=\"macro\")\n",
    "print(precision)\n",
    "\n",
    "\n",
    "recall = recall_metric.compute(average=\"macro\")\n",
    "print(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric \n",
    "\n",
    "metric_b = evaluate.load(\"bleu\")\n",
    "for batch in test_dataloader:  # Consider using a validation dataloader\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=3,  # Beam search with 2 beams\n",
    "#                 min_length=1,  # Set a min length if desired\n",
    "#                 max_length=2,  # Set a max length if desired\n",
    "                # Add more generation parameters if needed\n",
    "            )\n",
    "    \n",
    "    input_ques = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "    decoded_labels = [tokenizer.decode(ids, skip_special_tokens=True) for ids in labels]\n",
    "    decoded_outputs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n",
    "    \n",
    "    references = decoded_labels\n",
    "    predictions = decoded_outputs\n",
    "    metric_b.add_batch(predictions=predictions, references=references)\n",
    "    \n",
    "        # Display decoded texts for verification (for the first example in the batch)\n",
    "#     for i in range(bs):\n",
    "#         print(\"Input: \", input_ques[i])\n",
    "#         print(\"Label: \", decoded_labels[i])\n",
    "#         print(\"Output: \", decoded_outputs[i])\n",
    "#         print()\n",
    "#     break\n",
    "\n",
    "score_b = metric_b.compute()\n",
    "print(score_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.14927265620147687, 'rouge2': 0.008849277894255509, 'rougeL': 0.14889841463764086, 'rougeLsum': 0.14915796423343156}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric \n",
    "import evaluate\n",
    "metric = evaluate.load('rouge')\n",
    "for batch in test_dataloader:  # Consider using a validation dataloader\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=3,  # Beam search with 2 beams\n",
    "#                 min_length=1,  # Set a min length if desired\n",
    "#                 max_length=2,  # Set a max length if desired\n",
    "                # Add more generation parameters if needed\n",
    "            )\n",
    "    \n",
    "    input_ques = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "    decoded_labels = [tokenizer.decode(ids, skip_special_tokens=True) for ids in labels]\n",
    "    decoded_outputs = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n",
    "    \n",
    "    references = decoded_labels\n",
    "    predictions = decoded_outputs\n",
    "    metric.add_batch(predictions=predictions, references=references)\n",
    "    \n",
    "        # Display decoded texts for verification (for the first example in the batch)\n",
    "#     for i in range(bs):\n",
    "#         print(\"Input: \", input_ques[i])\n",
    "#         print(\"Label: \", decoded_labels[i])\n",
    "#         print(\"Output: \", decoded_outputs[i])\n",
    "#         print()\n",
    "#     break\n",
    "\n",
    "score = metric.compute()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
